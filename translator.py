import os
import time
import re
from pathlib import Path
from typing import List
from collections import deque
from openai import OpenAI

# ç¿»è¨³ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
TRANSLATION_SYSTEM_PROMPT = """ã‚ãªãŸã¯è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸ã®å°‚é–€ç¿»è¨³è€…ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ç¿»è¨³ã—ã¦ãã ã•ã„ï¼š

- è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹
- Markdownã®æ§‹æ–‡ï¼ˆè¦‹å‡ºã—ã€ç®‡æ¡æ›¸ãã€å¼·èª¿ã€ãƒªãƒ³ã‚¯ãªã©ï¼‰ã¯ä¿æŒã™ã‚‹
- Markdownæ§‹æ–‡ãŒå£Šã‚Œã¦ã„ã‚‹å ´åˆã¯æ­£ã—ãä¿®æ­£ã™ã‚‹
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```ï¼‰å†…ã®ã‚³ãƒ¼ãƒ‰ã¯ç¿»è¨³ã—ãªã„ï¼ˆãŸã ã—ã€ã‚³ãƒ¡ãƒ³ãƒˆã¯ç¿»è¨³ã™ã‚‹ï¼‰
- Markdownã®è¦‹å‡ºã—1ã¨Pythonã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒ¡ãƒ³ãƒˆã®åŒºåˆ¥ã«æ³¨æ„ã—ã¦ãã ã•ã„
- æ•°å¼ï¼ˆ$...$ã€$...$ï¼‰ã¯ç¿»è¨³ã—ãªã„
- ç”»åƒãƒªãƒ³ã‚¯ï¼ˆ![]()ï¼‰ã¯ç¿»è¨³ã—ãªã„

ç¿»è¨³çµæœã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

class MarkdownChunker:
    """Markdownãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’æ‹…å½“ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_chunk_size: int = 5000, min_chunk_size: int = 500):
        """
        ãƒãƒ£ãƒ³ã‚«ãƒ¼ã®åˆæœŸåŒ–
        
        Args:
            max_chunk_size: ãƒãƒ£ãƒ³ã‚¯æœ€å¤§æ–‡å­—æ•°
            min_chunk_size: ãƒãƒ£ãƒ³ã‚¯æœ€å°æ–‡å­—æ•°
        """
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size

    def split_markdown(self, markdown_text: str) -> List[str]:
        """Markdownãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ç”¨ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        if len(markdown_text) <= self.max_chunk_size:
            return [markdown_text]
        
        print("ğŸ“ Markdownãƒ˜ãƒƒãƒ€ãƒ¼ã§åˆ†å‰²ä¸­...")

        # re.split ã‚’ä½¿ã£ã¦ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã”ã¨ã«åˆ†å‰²ã—ã¦ã€ãã‚Œãã‚Œã«å…ƒã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä»˜ã‘ç›´ã™
        chunks = re.split(r'(?=^#{1,6} .*)', markdown_text, flags=re.MULTILINE)

        # ç©ºæ–‡å­—åˆ—ãŒæ··ã–ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹
        chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
        
        print(f"ğŸ“Š ãƒ˜ãƒƒãƒ€ãƒ¼åˆ†å‰²å¾Œ: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")
        
        # å°ã•ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆ
        chunks = self._merge_small_chunks(chunks)
        
        print(f"âœ… åˆ†å‰²å®Œäº†: ç·ãƒãƒ£ãƒ³ã‚¯æ•° {len(chunks)}å€‹")
        return chunks
    

    def _merge_small_chunks(self, chunks: List[str]) -> List[str]:
        """å°ã•ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
        # ãƒãƒ£ãƒ³ã‚¯ãŒ1ã¤ã—ã‹ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if len(chunks) <= 1:
            return chunks
        
        n_chunks = len(chunks)
        chunks = deque(chunks)

        merged_chunks = []
        while chunks:
            chunk = chunks.popleft()
            if len(chunk) > self.min_chunk_size:
                merged_chunks.append(chunk)
            elif chunks:
                chunks[0] = chunk + "\n\n" + chunks[0]  # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã¨çµåˆ
            else:
                merged_chunks[-1] += "\n\n" + chunk  # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã¨çµåˆ
                
        print(f"âœ… ç·çµåˆå›æ•°: {n_chunks - len(merged_chunks)}å›")    
        return merged_chunks


class MarkdownTranslator:
    """Markdownç¿»è¨³ã‚’æ‹…å½“ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str = None, model: str = "gpt-4.1-mini", 
                 max_chunk_size: int = 5000, min_chunk_size: int = 500):
        """
        Markdownç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        
        Args:
            api_key: OpenAI APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            model: ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«
            max_chunk_size: ãƒãƒ£ãƒ³ã‚¯æœ€å¤§æ–‡å­—æ•°
            min_chunk_size: ãƒãƒ£ãƒ³ã‚¯æœ€å°æ–‡å­—æ•°
        """
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã¾ãŸã¯å¼•æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’æ‹…å½“ã™ã‚‹ã‚¯ãƒ©ã‚¹
        self.chunker = MarkdownChunker(max_chunk_size, min_chunk_size)

    def translate_chunk(self, chunk: str, chunk_index: int, total_chunks: int) -> str:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã‚’ç¿»è¨³"""
        try:
            print(f"ğŸŒ ç¿»è¨³ä¸­ ({chunk_index + 1}/{total_chunks}) - {len(chunk)}æ–‡å­—...")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": TRANSLATION_SYSTEM_PROMPT},
                    {"role": "user", "content": chunk}
                ],
                temperature=0,
                max_tokens=None  # gpt-4.1-miniã¯è‡ªå‹•ã§æœ€å¤§ã¾ã§ä½¿ç”¨
            )
            
            translated = response.choices[0].message.content.strip()
            
            # APIã®ä½¿ç”¨é‡æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            usage = response.usage
            print(f"  âœ… å®Œäº† - å…¥åŠ›: {usage.prompt_tokens}tokens, å‡ºåŠ›: {usage.completion_tokens}tokens")
            
            return translated
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿”ã™ï¼ˆç¿»è¨³å¤±æ•—ã§ã‚‚å‡¦ç†ç¶šè¡Œï¼‰
            return chunk

    def translate_file(self, input_path: str, output_path: str = None, delay: float = 1.0) -> str:
        """
        Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¿»è¨³
        
        Args:
            input_path: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            delay: APIå‘¼ã³å‡ºã—é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            ç¿»è¨³å¾Œã®Markdownãƒ†ã‚­ã‚¹ãƒˆ
        """
        input_file = Path(input_path)
        if not input_file.exists():
            raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
        
        # å‡ºåŠ›ãƒ‘ã‚¹ã®è‡ªå‹•ç”Ÿæˆ
        if output_path is None:
            output_path = input_file.parent / f"{input_file.stem}_ja{input_file.suffix}"
        
        print(f"ğŸ“– ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {input_path}")
        markdown_text = input_file.read_text(encoding="utf-8")
        print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(markdown_text):,}æ–‡å­—")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆMarkdownChunkerã«å§”è­²ï¼‰
        chunks = self.chunker.split_markdown(markdown_text)
        
        if len(chunks) == 1:
            print("ğŸ“„ åˆ†å‰²ä¸è¦ï¼ˆå˜ä¸€ãƒãƒ£ãƒ³ã‚¯ï¼‰")
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ç¿»è¨³
        translated_chunks = []
        start_time = time.time()
        
        for i, chunk in enumerate(chunks):
            translated_chunk = self.translate_chunk(chunk, i, len(chunks))
            translated_chunks.append(translated_chunk)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆæœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ä»¥å¤–ã§å¾…æ©Ÿï¼‰
            if i < len(chunks) - 1 and delay > 0:
                time.sleep(delay)
        
        # ç¿»è¨³çµæœã‚’çµåˆ
        translated_markdown = "\n\n".join(translated_chunks)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_file = Path(output_path)
        output_file.write_text(translated_markdown, encoding="utf-8")
        
        # çµæœã‚µãƒãƒªãƒ¼
        elapsed_time = time.time() - start_time
        print("\n" + "="*50)
        print("ğŸ‰ ç¿»è¨³å®Œäº†!")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
        print(f"ğŸ“Š ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
        print(f"â±ï¸  æ‰€è¦æ™‚é–“: {elapsed_time:.1f}ç§’")
        print(f"ğŸ“ˆ ç¿»è¨³å¾Œã‚µã‚¤ã‚º: {len(translated_markdown):,}æ–‡å­—")
        print("="*50)
        
        return translated_markdown